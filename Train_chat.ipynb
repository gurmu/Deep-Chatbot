{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMu+rxHxV/2egL95vg+4Mvu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gurmu/Deep-Chatbot/blob/master/Train_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwXwadzBcd0Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f966e5f-ff60-4d25-9d4b-68a403da4f1b"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNZvQIb2clAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUA25ntrcr4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path):\n",
        "    \"\"\"\n",
        "    Reads the data from the csv file.\n",
        "    Arguments:\n",
        "        path: a string.\n",
        "    Returns:\n",
        "        dataset: a Pandas Dataframe with two columns, namely: `Question` and `Answer`.\n",
        "        X: a NumPy array representing `Question` column.\n",
        "        Y: a NumPy array representing `Answer` column.\n",
        "    \"\"\"\n",
        "    \n",
        "    # read the csv file into a pandas dataframe\n",
        "    dataset = pd.read_csv(path, usecols=['Question', 'Answer'])\n",
        "    # make sure all cell values are strings; because some of them \n",
        "    # only contain numbers, so they maybe mistaken with other types.\n",
        "    dataset = dataset.applymap(str)\n",
        "    # shuffle the rows of the dataframe and then reset the index\n",
        "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "    \n",
        "    X = np.asarray(dataset['Question'])\n",
        "    Y = np.asarray(dataset['Answer'])\n",
        "    \n",
        "    X = np.apply_along_axis(lambda sen: '<start> '+ sen + ' <end>', 0, X)\n",
        "    Y = np.apply_along_axis(lambda sen: '<start> '+ sen + ' <end>', 0, Y)\n",
        "    \n",
        "    return dataset, X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6paNfgwdA6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96e56cea-6b00-4cdc-9403-80d400945cf0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZniusXVeBT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7pHSuNseHc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7816440c-f30a-4c9a-d44b-f97558198774"
      },
      "source": [
        "%cd path/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/path\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AgYfXuXeK8P",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "f3f8634f-ddfb-4bb4-8621-f2c3ec5f1d26"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6ab5aa45-883f-42a3-9e82-caa5fedbede6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-6ab5aa45-883f-42a3-9e82-caa5fedbede6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving bot.csv to bot.csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bot.csv': b',Question,Answer\\r\\n0,do i have too many issues for counseling ?,there is no such thing as too many issues for counseling . many issues are often interrelated and can all be worked on with some time and patience .\\r\\n1,how can i find myself again ?,check this blog out : four - ways - add - self - esteem - friends - listhope you find a few nuggets of helpfulness in this .\\r\\n2,is it normal to go into therapy feeling nervous ?,certainly .\\r\\n3,i have difficulty with communication,\"i can offer you hypnosis for confidence in presentations , via skype , if you \\' re in cali .\"\\r\\n4,how do i become more self - confident in general ?,check out my latest blog : four - ways - add - self - esteem - friends - listi hope this offers you some nuggets of helpfulness !\\r\\n5,why do i have low self - esteem and lack confidence ?,check out my latest blog on : four - ways - add - self - esteem - friends - listi hope this offers you some nuggets of helpfulness !\\r\\n6,i hate the way i look,check out my latest blog : four - ways - add - self - esteem - friends - listi hope this offers you some nuggets of helpfulness !\\r\\n7,i want to be a boy but i can \\' t because of my religion,\"chances are your family already knows , they are probably just waiting on confirmation from you to say it . a parent knows their child .\"\\r\\n8,\"i cheated on my partner , and i don \\' t know how to feel\",my questions to you would be : why did you get married ? would it bother you if your spouse did the same to you ?\\r\\n9,what do you do if your partner isn \\' t satisfying your needs sexually ?,depends : what do would you like to see happen ?\\r\\n10,i just lost my grandpa,your local hospice will have grief support groups and free community counseling available with bereavement counselors who are expects with grief and loss .\\r\\n11,i over endulge when i drink alcohol and feel extremely guilty about it the next day,i offer that getting a professional assessment is in order to look at your relationship with alcohol .\\r\\n12,how can i avoid family members who stress me out ?,i just want to understand before i answer . who exactly is complaining ?\\r\\n13,what \\' s wrong with me for going to summer school ?,there is nothing wrong with going to summer school .\\r\\n14,i feel like i could never be with anyone because no one would want me,what would make you feel no one wants to be with you ?\\r\\n15,how can i be interested in the same sex after nearly 40 years of being straight ?,sexuality is fluid . it is possible to find yourself attracted sexually or affectionally to different types of people at different times in your life .\\r\\n16,how do i go about asking my ex - girlfriend to expose me to her friends so i can come out finally ?,i am a bit confused ? are your ex - girlfriend \\' s friends gay ? i feel the need for a bit more information .\\r\\n17,how does counseling end ?,for most : when the money / insurance runs out . when best : when the job is done . . . and you \\' re feeling much better .\\r\\n18,how can i be less stressed ?,learn how to meditate . i recommend a mindful based stress reduction mbsr program .\\r\\n19,why do i get aroused when women call me \\' gay \\' ?,maybe it is a way to secretly satisfy your own wish to be or to live life gay .\\r\\n20,how does a counselor diagnose someone with a disorder ?,\"a lot goes into diagnosing for a disorder . it is according to what the provider \\' s assessment from information gathered , reported and observed from the client .\"\\r\\n21,how would i know if i have the right therapist ?,if you connect with your therapist and can say whatever you want to say and not be judged .\\r\\n22,is it normal to cry at therapy ?,crying is clearing . it is a release of energy and toxins . crying is a good thing and a great way to let go and move forward !\\r\\n23,is it normal to cry at therapy ?,\"for many people , crying is a stress - release valve . it is perfectly ok to cry in counseling !\"\\r\\n24,how does someone approach a counselor ?,\"usually people call me by phone , they introduce themselves , we chat for a bit , then we schedule a time for their appointment\"\\r\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01oyGCSaflzB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f6c19945-d174-4dea-8755-3c714b72e1c9"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bot.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ3SiXEZf4iD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18c2f9f2-2046-4675-91b3-b2029d912dc7"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bot.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihe9OJmcf64H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path):\n",
        "    \"\"\"\n",
        "    Reads the data from the csv file.\n",
        "    Arguments:\n",
        "        path: a string.\n",
        "    Returns:\n",
        "        dataset: a Pandas Dataframe with two columns, namely: `Question` and `Answer`.\n",
        "        X: a NumPy array representing `Question` column.\n",
        "        Y: a NumPy array representing `Answer` column.\n",
        "    \"\"\"\n",
        "    \n",
        "    # read the csv file into a pandas dataframe\n",
        "    dataset = pd.read_csv(path, usecols=['Question', 'Answer'])\n",
        "    # make sure all cell values are strings; because some of them \n",
        "    # only contain numbers, so they maybe mistaken with other types.\n",
        "    dataset = dataset.applymap(str)\n",
        "    # shuffle the rows of the dataframe and then reset the index\n",
        "    dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
        "    \n",
        "    X = np.asarray(dataset['Question'])\n",
        "    Y = np.asarray(dataset['Answer'])\n",
        "    \n",
        "    X = np.apply_along_axis(lambda sen: '<start> '+ sen + ' <end>', 0, X)\n",
        "    Y = np.apply_along_axis(lambda sen: '<start> '+ sen + ' <end>', 0, Y)\n",
        "    \n",
        "    return dataset, X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33_wGvQHgGJy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a0e3d37a-af46-43c0-9c19-7f4a9f79cbac"
      },
      "source": [
        "files_path = '/content/path/'\n",
        "qa_dataframe, X, Y = load_dataset(files_path + 'bot.csv')\n",
        "qa_dataframe.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>why do i have low self - esteem and lack confi...</td>\n",
              "      <td>check out my latest blog on : four - ways - ad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>do i have too many issues for counseling ?</td>\n",
              "      <td>there is no such thing as too many issues for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>is it normal to go into therapy feeling nervous ?</td>\n",
              "      <td>certainly .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>how can i avoid family members who stress me o...</td>\n",
              "      <td>i just want to understand before i answer . wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>how does counseling end ?</td>\n",
              "      <td>for most : when the money / insurance runs out...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Question                                             Answer\n",
              "0  why do i have low self - esteem and lack confi...  check out my latest blog on : four - ways - ad...\n",
              "1         do i have too many issues for counseling ?  there is no such thing as too many issues for ...\n",
              "2  is it normal to go into therapy feeling nervous ?                                        certainly .\n",
              "3  how can i avoid family members who stress me o...  i just want to understand before i answer . wh...\n",
              "4                          how does counseling end ?  for most : when the money / insurance runs out..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8BMDsnTgX0L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b44839d7-fc73-488d-93fd-628682fed2d2"
      },
      "source": [
        "print(f\"Number of question-answer pairs in the dataset: {len(qa_dataframe)}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of question-answer pairs in the dataset: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4S_XorZgrdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwpCo9aXgvnz",
        "colab_type": "text"
      },
      "source": [
        "Cache all hyperparameters into this dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNuKu-IEgwrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameters = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6G8uKykg3CJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub75Qe8ohAs2",
        "colab_type": "text"
      },
      "source": [
        "**Choose the size of vocabulary:**\n",
        "\n",
        "This is a hyperparameter that you can play with.![alt text](![alt text](https://))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhUVAjnghD_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kxxwwSehUMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJpn0vtShcOB",
        "colab_type": "text"
      },
      "source": [
        "Now, let's tokenize the data, by converting each sentence (question or answer) to a sequence of integers which represent indices of their embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM2hxr87hdpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(sentences, vocab_size):\n",
        "  \n",
        "    \"\"\"\n",
        "    Using Tensorflow Tokenizer to turn each sentence into a sequence of integers \n",
        "    (each integer being the index of a token in a dictionary).\n",
        "    Arguments:\n",
        "      X: a list or a NumPy array of strings, where each element is a sentence.\n",
        "    Returns:\n",
        "      tensor: a NumPy ndarray, where each row represents the the sequence of\n",
        "              integers that maps the words of the equivalent sentence in \n",
        "              the `sentences` list to a their indices (for embbedings). shape=(batch_size, )\n",
        "      lang_tokenizer: a Tensorflow Tokenizer which have been fit on `sentences`.\n",
        "    \"\"\" \n",
        "\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, \n",
        "                                                           filters='')\n",
        "    lang_tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(sentences)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G24oFTAohrQ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "19912210-de7f-48ec-e19a-d803753d22ef"
      },
      "source": [
        "# all sentences (Questions & Answers)\n",
        "# we need to fit the tokenizer on all sentences; to create word_index and \n",
        "# index_word mapper dictionaries for the most frequent VOCAB_SIZE= 10,000 words.\n",
        "texts = np.concatenate((X, Y))\n",
        "# tokenize the data\n",
        "tensor, text_tokenizer = tokenize(texts, VOCAB_SIZE)\n",
        "\n",
        "print(f\"texts[0]= {texts[0]}\")\n",
        "print(f\"tensor[0]= {tensor[0]}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "texts[0]= <start> why do i have low self - esteem and lack confidence ? <end>\n",
            "tensor[0]= [ 1 22  3  4 11  7  2  0  0  0  0  0  0  0  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhfCEVMzhsW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNJNeMsfhyNR",
        "colab_type": "text"
      },
      "source": [
        "As we see, each sentence has been converted to an array with indices that will be used to map words to their embedding vectors.\n",
        "\n",
        "Now, let's extract back X (for questions) and Y (for answers) arrays from tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRzirts8hzhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9457f19b-019d-4204-aa48-805fb34d0563"
      },
      "source": [
        "# extract questions and answers back from tensor\n",
        "X, Y = tensor[:len(X)], tensor[len(X):]\n",
        "\n",
        "print(f\"Shape of X (questions): {X.shape}\")\n",
        "print(f\"Shape of Y (answers): {Y.shape}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X (questions): (25, 15)\n",
            "Shape of Y (answers): (25, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsK-HgPgiEEt",
        "colab_type": "text"
      },
      "source": [
        "Get the maximum sequence length for both input and target tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws7PmTiWiFU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_seq_length(tensor):\n",
        "    \"\"\"\n",
        "    Get maximum sequence length in the corpus. And, make sure that all rows in\n",
        "    `tensor` has the same length.\n",
        "    Arguments:\n",
        "      tensor: a NumPy ndarray of shape (batch_size,) where each row represents\n",
        "      indices mapping to words in equivalent sentences.\n",
        "    Returns:\n",
        "      max_len: an integer representing maximum sequence length.\n",
        "    \"\"\"\n",
        "    batch_size = len(tensor)\n",
        "    lengths = [len(sentence) for sentence in tensor]\n",
        "    max_len = max(lengths)\n",
        "\n",
        "    # check if all rows in `tensor` has the same length (equal to max_len)\n",
        "    assert lengths == [max_len]*batch_size\n",
        "\n",
        "    return max_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTO75I1diPXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9d955cb8-cfe6-454c-b5a9-183f1264bd4f"
      },
      "source": [
        "# Get max_seq_length of input and target tensors\n",
        "max_length_inp, max_length_targ = max_seq_length(X), max_seq_length(Y)\n",
        "print(f\"Maximum sequence length for input (questions) tensor: {max_length_inp}\")\n",
        "print(f\"Maximum sequence length for target (answers) tensor: {max_length_targ}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length for input (questions) tensor: 15\n",
            "Maximum sequence length for target (answers) tensor: 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSiVJO3OiUPm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gMmlLxxicY9",
        "colab_type": "text"
      },
      "source": [
        "Save the arrays as npy files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96GRRVEaieAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_arrays_path = os.path.join(files_path, 'data_arrays')\n",
        "# create the folder if it does not exist\n",
        "if not os.path.exists(data_arrays_path):\n",
        "    os.makedirs(data_arrays_path)\n",
        "    \n",
        "np.save(os.path.join(data_arrays_path, 'X.npy'), X)\n",
        "np.save(os.path.join(data_arrays_path, 'Y.npy'), Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t26ox1stimTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrChK50xit92",
        "colab_type": "text"
      },
      "source": [
        "## Download GloVe Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAVO4HqXiwkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "948b18d3-f236-41c0-924f-2b803a5af0c7"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-04 11:56:33--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-05-04 11:56:33--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-05-04 11:56:33--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.01MB/s    in 6m 28s  \n",
            "\n",
            "2020-05-04 12:03:02 (2.12 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMmiHhcgi3e2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8263e4db-5c9c-4b95-8f3d-9ed599094869"
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZQoe-yfkiPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d3e30b2d-6673-4b75-d451-9a551ef059ae"
      },
      "source": [
        "!ls\n",
        "!pwd"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bot.csv      glove.6B.100d.txt\tglove.6B.300d.txt  glove.6B.zip\n",
            "data_arrays  glove.6B.200d.txt\tglove.6B.50d.txt\n",
            "/content/path\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpIp-UAQk0zZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-2AijkUk928",
        "colab_type": "text"
      },
      "source": [
        "Read GloVe word embeddings into a word to vector dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGxxWZa5k_Hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_glove_vectors(glove_file):\n",
        "    \"\"\"\n",
        "    This function reads GloVe vectors from .txt file and \n",
        "    returns a word to vector dictionary.\n",
        "    Arguments:\n",
        "      glove_file: a string path to GloVe word embeddings file.\n",
        "    Returns:\n",
        "      word_to_vec: a Python dictionary that maps words to their embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    import numpy as np\n",
        "    \n",
        "    # open the file\n",
        "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
        "        \n",
        "        words = set()\n",
        "        word_to_vec = {}\n",
        "\n",
        "        # loop over the rows in the file\n",
        "        for line in f:\n",
        "            # read the line, strip it (remove leading and trailing spaces) and split it\n",
        "            line = line.strip().split()\n",
        "            # first item in the list 'line' is the word itself\n",
        "            curr_word = line[0]\n",
        "            # add the word to set of words\n",
        "            words.add(curr_word)\n",
        "            # add the words with its vector representation as a (key, value) pair to the dictionary\n",
        "            word_to_vec[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "\n",
        "    return word_to_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDjDMXBRlHqM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_file = \"/content/path/glove.6B.200d.txt\"\n",
        "word_to_vec = read_glove_vectors(glove_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFIhMwqLlfcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu4RbVNgln5_",
        "colab_type": "text"
      },
      "source": [
        "# The Model\n",
        "Choosing Hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzHHwwLTlqmj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = X.shape[0]\n",
        "BATCH_SIZE = 10\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE \n",
        "embedding_dim = 20\n",
        "units = 15 \n",
        "vocab_size = VOCAB_SIZE "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Q6SGvNBmCWF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "28a8ff08-2735-4076-84c6-138052ef9f78"
      },
      "source": [
        "print(f\"Buffer size: {BUFFER_SIZE}, Batch size: {BATCH_SIZE}, Steps per epoch: {steps_per_epoch}\")\n",
        "print(f\"Embedding size: {embedding_dim}, # of units: {units}\")\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Buffer size: 25, Batch size: 10, Steps per epoch: 2\n",
            "Embedding size: 20, # of units: 15\n",
            "Vocab size: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMs5ENdgmKT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cache these values into the hyperparameters dictionary\n",
        "hyperparameters['buffer_size'] = BUFFER_SIZE\n",
        "hyperparameters['batch_size'] = BATCH_SIZE\n",
        "hyperparameters['steps_per_epoch'] = steps_per_epoch\n",
        "hyperparameters['embedding_dim'] = embedding_dim\n",
        "hyperparameters['units'] = units\n",
        "hyperparameters['vocab_size'] = vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_Rg1CUkmWib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-irtO98me1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTRInvLcmhvL",
        "colab_type": "text"
      },
      "source": [
        "Create the embedding matrix for words in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHfFENfxmit5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_embedding_matrix(words, word_to_vec, vocab_size, embedding_dim):\n",
        "  \"\"\"\n",
        "  Returns an embeddings matrix for the words in the vocabulary.\n",
        "  Arguments:\n",
        "      words: a list of words.\n",
        "      word_to_vec: a dictionary that maps words to their embedding vectors.\n",
        "      vocab_size: an integer which represents the size of the vocabulary.\n",
        "      emb_dim: an integer which represents the dimension of word embeddings.\n",
        "  Returns:\n",
        "      embedding_matrix: a NumPy array with shape of (vocab_size, emb_dim).\n",
        "  \"\"\"\n",
        "\n",
        "  # create embedding matrix\n",
        "  embedding_matrix = np.zeros((vocab_size, embedding_dim), dtype=np.float64)\n",
        "\n",
        "  # loop over the words in our vocabulary\n",
        "  for i, word in enumerate(words):\n",
        "    if word in word_to_vec.keys():\n",
        "      # if the current word is in glove vocab, get its glove vector.\n",
        "      embedding_matrix[i, :] = word_to_vec[word]\n",
        "    else:\n",
        "      # if the current word does not exist in the vocabulary, set its vector to zeros.\n",
        "      embedding_matrix[i, :] = np.zeros((embedding_dim,), dtype=np.float64)\n",
        "\n",
        "  return embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13VizqOempSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# since Tensorflow tokenizer word_index still have all words even when \n",
        "# vocab_size is passed while defining the Tokenizer. \n",
        "# So, we need to grab the first 10,000 words.\n",
        "words = list(text_tokenizer.word_index.keys())[:vocab_size]\n",
        "embedding_matrix = create_embedding_matrix(words, word_to_vec, vocab_size, \n",
        "                                           embedding_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CmdhrVEmvZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_vec.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF9VoFnTx2UT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD7IdPrtx5mZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23bc0bdf-bff5-409d-e275-b92145fca208"
      },
      "source": [
        "embedding_dim"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzXgOAswyqyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKq701ntyzNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "curr_word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8P27ApV4hXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kpvlqnbj4k9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emb_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jMDZIDX62NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoeXuFzilcxL",
        "colab_type": "text"
      },
      "source": [
        "Create a tf.data dataset for X and Y with buffer size equal to BUFFER_SIZE and batch size equal to BATCH_SIZE, we will use this dataset to generate batches while training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mIGJ_fnleD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxQUrSbrlle2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z46cJuxmlvSC",
        "colab_type": "text"
      },
      "source": [
        "Example of an input batch that the model will receive while iterating over dataset batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlJfBG8TlwXV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a2e7d2b-9502-445c-8449-c07d19776d5e"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([10, 15]), TensorShape([10, 15]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qs2MwI67l2Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRHPG9RrtHBC",
        "colab_type": "text"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB8eU1UctJRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXBne1AttOqn",
        "colab_type": "text"
      },
      "source": [
        "I'm going to use an encoder-decoder model with Bahdanau's attention, which has been described in detail in this paper:\n",
        "\n",
        "Effective Approaches to Attention-based Neural Machine Translation\n",
        "\n",
        "Some properties of the model:\n",
        "\n",
        "The encoder will be a bidirectional encoder with 512 hidden units for each direction, resulting in 1024 cells.\n",
        "\n",
        "Both encoder & decoder will use LSTM as the cell.\n",
        "\n",
        "The decoder will be unidirectional with 1024 hidden units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eats4TZetPm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZSY8jX6tbJT",
        "colab_type": "text"
      },
      "source": [
        "## The Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8limwpotc3Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyBY0u_TtjZE",
        "colab_type": "text"
      },
      "source": [
        "Define the bidirectional encoder architecture which will consist of the following:\n",
        "\n",
        "An Embedding layer that will map input sentences to their embeddings. The vocabulary size is equal to VOCAB_SIZE=10,000 and the dimension of embedding is 200 (Note: although a power of 2 embedding size would be more suitable to speed up training time by increasing cache utilization during data movement, thus reducing bottlenecks, but GloVe embeddings do not come with a power of 2 embedding size).\n",
        "\n",
        "A forward & backward (bidirectional) LSTM layer with 512 units for each.\n",
        "\n",
        "The input to the encoder has a shape of (batch_size, ), which is the input array of sentences (questions).\n",
        "\n",
        "The hidden state arrays of the encoder are all of the shape (batch_size, n_units).\n",
        "\n",
        "The output of the encoder has a shape of (batch_size, max_sequence_length, n_units x2) ;the hidden size of the output is equal to double the size of units since the encoder is bidirectional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4Ina7rltktX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, \n",
        "                 batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
        "                                                   trainable=True)\n",
        "        self.bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(self.enc_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform'))\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Propogates the input `x` through the bidirectional encoder and returns\n",
        "        the outputs along with hidden states.\n",
        "        Arguments:\n",
        "            x: a tensor with shape (batch_size, max_seq_length)\n",
        "            hidden: a tuple or a list, with four tensors representing hidden\n",
        "                    and memory states for both the forward and backward LSTMs.\n",
        "                    Each of them having a shape of (batch_size, max_seq_length)\n",
        "        Returns:\n",
        "            output: a tensor representing the output of the encoder with a shape\n",
        "                    of (batch size, max_sequence length, units*2)\n",
        "            state_h: a tensor, with forward and backward hidden states of \n",
        "                     the encoder with a shape of (batch size, units*2)\n",
        "            state_c: a tensor, with forward and backward memory cell states of\n",
        "                     the encoder with a shape of (batch size, units*2)\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        output, fstate_h, fstate_c, bstate_h, bstate_c = self.bi_lstm(x, initial_state = hidden)\n",
        "\n",
        "        state_h = tf.keras.layers.Concatenate()([fstate_h, bstate_h])\n",
        "        state_c = tf.keras.layers.Concatenate()([fstate_c, bstate_c])\n",
        "\n",
        "        return output, state_h, state_c\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        # forward_state_h forward hidden state output, backward_state_h backward hidden state output\n",
        "        # forward_state_c forward cell (memory) state output, backward_state_c backward cell (memory) state output\n",
        "        return (tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units)), \n",
        "                tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4CKdlLkvdmt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DKJqZB2vjY-",
        "colab_type": "text"
      },
      "source": [
        "Define the encoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_Y6gW3ovkaf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9538ecae-3e8b-4daa-9d7f-fdcbd5e4ee27"
      },
      "source": [
        "encoder = Encoder(vocab_size, embedding_dim, units, \n",
        "                  BATCH_SIZE)\n",
        "\n",
        "# usage example\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
        "print (f'Encoder output shape: (batch size, sequence length, units*2) {sample_output.shape}')\n",
        "print (f'Encoder Hidden state shape: (batch size, units*2) {sample_h.shape}')\n",
        "print (f'Encoder Memory state shape: (batch size, units*2) {sample_c.shape}')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units*2) (10, 15, 30)\n",
            "Encoder Hidden state shape: (batch size, units*2) (10, 30)\n",
            "Encoder Memory state shape: (batch size, units*2) (10, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbcwxplnvsCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfkaFnIfw8R1",
        "colab_type": "text"
      },
      "source": [
        "# Bahdanau Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64xWKXSXw-wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  \n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units*2)\n",
        "        self.W2 = tf.keras.layers.Dense(units*2)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # query hidden state shape == (batch_size, hidden size)\n",
        "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # values shape == (batch_size, max_len, hidden size)\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)\n",
        "        \n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuUCQ37szVOI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaZ7gVFlzcSw",
        "colab_type": "text"
      },
      "source": [
        "Example of using Bahdanau's attention:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBc1YkaQzdYH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8d60b95d-f41c-468b-8c5c-bf4bd3824ecf"
      },
      "source": [
        "# usage example\n",
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_h, sample_output)\n",
        "\n",
        "print(f\"Attention result shape: (batch size, units*2) {attention_result.shape}\")\n",
        "print(f\"Attention weights shape: (batch_size, sequence_length, 1) {attention_weights.shape}\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units*2) (10, 30)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (10, 15, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b50zi2n4zwBU",
        "colab_type": "text"
      },
      "source": [
        "## The Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tabTvjPQz6Ha",
        "colab_type": "text"
      },
      "source": [
        "Define the decoder architecture which will consist of the following:\n",
        "\n",
        "Bahdanau's attention which is applied to the output of the encoder to get the context which will be passed, along with the embeddings of the decoder input, to the LSTM layer of the decoder.\n",
        "\n",
        "An Embedding layer with a vocabulary size of VOCAB_SIZE=10,000 words and an embedding dimension of 200. The embedding takes the decoder input which is of the shape (batch_size, ) and outputs a tensor of the shape (batch_size, 1, embedding_dim).\n",
        "\n",
        "A forward (unidirectional) LSTM layer with 1024 units for each.\n",
        "\n",
        "The shape of the decoder's output is (batch_size, vocab_size) and the decoder hidden state size is of the shape (batch_size, units x2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PEr7pF9z7Hu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, \n",
        "                 batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                                   trainable=True)\n",
        "        self.lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                       return_sequences=True,\n",
        "                                       return_state=True,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        \"\"\"\n",
        "        Takes decoder input, hidden state and output of the encoder and\n",
        "        returns the decoder's predictions along with decoder hidden state and\n",
        "        attention weights.\n",
        "        Arguments:\n",
        "            x: a tensor with shape (batch_size, 1) which is the decoder input\n",
        "               at some timestep.\n",
        "            hidden: a tensor representing hidden state of the encoder with a\n",
        "                    shape of (batch size, units*2)\n",
        "            enc_output: a tensor representing the encoder's output with a shape\n",
        "                        of (batch size, sequence length, units*2)\n",
        "                        Returns:\n",
        "            x: a tensor with shape (batch_size, vocab size) and it's decoder's\n",
        "               output.\n",
        "            state: a tensor that represents the hidden state of the decoder \n",
        "                   and has a shape of (batch_size, units*2)\n",
        "            attention_weights: a tensor that represents attention weights and \n",
        "                               has a shape of (batch_size, sequence_length, 1)\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the LSTM\n",
        "        output, state_h, state_c = self.lstm(x)\n",
        "        state = state_h\n",
        "\n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "\n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKascqpN1PGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3ReEqRh1VV-",
        "colab_type": "text"
      },
      "source": [
        "Define the decoder:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VHJD2o_1WtL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f6842531-5be1-4f3f-f661-787a6a82b8d2"
      },
      "source": [
        "decoder = Decoder(vocab_size, embedding_dim, units*2, \n",
        "                  BATCH_SIZE)\n",
        "\n",
        "# usage example\n",
        "sample_decoder_output, dec_h, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_h, sample_output)\n",
        "\n",
        "print(f'Decoder output shape: (batch_size, vocab size) {sample_decoder_output.shape}')\n",
        "print(f'Decoder hidden state shape: (batch_size, units*2) {dec_h.shape}')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (10, 25)\n",
            "Decoder hidden state shape: (batch_size, units*2) (10, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrrmvodC1183",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XPc8Zp714c3",
        "colab_type": "text"
      },
      "source": [
        "## Defining Optimizer, Loss Function and Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj_tkb5Y16Zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ARCY--L1_5l",
        "colab_type": "text"
      },
      "source": [
        "Define the loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeUx6NP62A6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, \n",
        "                                                            reduction='none')\n",
        "\n",
        "def compute_loss(real, pred):\n",
        "    \"\"\"\n",
        "    This function returns the loss for model's predictions on a batch \n",
        "    of data in comparison with the real outputs at a timestep.\n",
        "    Arguments:\n",
        "        real: real output, a Tensorflow tensor with a shape \n",
        "              of: (batch_size, max_seq_length)\n",
        "        pred: model's predictions at a certain timestep, a Tensorflow tensor \n",
        "              with a shape of: (batch_size, max_seq_length)\n",
        "    Returns:\n",
        "        A Tensorflow tensor with the loss.\n",
        "    \"\"\"\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_BAgE0d2aAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xklszAUt2goD",
        "colab_type": "text"
      },
      "source": [
        "Define the perplexity metric:\n",
        "\n",
        "**Note**: I was going to incorporate BLEU score as another metric, but to compute BLEU score on a batch of data, it turns out that we need access the data to count the number of n-grams. In order to do that, we can't use Tensorflow's AutoGraph feature tf.function. Anyways, I tried it and using tf.function was around 2.5 times faster than dropping this feature only to compute BLEU score. In case you want to use BLEU score, you can use nmt's open source implementation which you can find here, or you can use NLTK's implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYOBRP--2j7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_perplexity(real, pred):\n",
        "    \"\"\"\n",
        "    This function returns the perplexity for model's predictions on a batch \n",
        "    of data in comparison with the real outputs at a timestep.\n",
        "    Arguments:\n",
        "        real: real output, a Tensorflow tensor with a shape \n",
        "              of: (batch_size, max_seq_length)\n",
        "        pred: model's predictions at a certain timestep, a Tensorflow tensor \n",
        "              with a shape of: (batch_size, max_seq_length)\n",
        "    Returns:\n",
        "        A Tensorflow tensor with the perplexity.\n",
        "    \"\"\"\n",
        "    \n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.cast(tf.pow(math.e, tf.keras.backend.mean(loss_, axis=-1)), \n",
        "                   dtype=tf.keras.backend.floatx())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxXgmtY25B_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88TeHgzL5Jks",
        "colab_type": "text"
      },
      "source": [
        "# Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1zIAor85LiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/content/path/training_checkpoints'\n",
        "\n",
        "# create the folder if it does not exist\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjhzJj1H5q89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2wBCMgq5y7A",
        "colab_type": "text"
      },
      "source": [
        "# TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKEDwdM750iU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5e04bced-9366-44fe-eedc-62472f18beb0"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEYxOpxq6La9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_file_name =f\"/content/path/chatbot/logs/metrics_{int(time.time())}\"\n",
        "\n",
        "# create the folder if it does not exist\n",
        "if not os.path.exists(log_file_name):\n",
        "    os.makedirs(log_file_name)\n",
        "    \n",
        "summary_writer = tf.summary.create_file_writer(log_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r3YkyX86g95",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gQV4x9z6mYC",
        "colab_type": "text"
      },
      "source": [
        "# Define training step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS0gOYqg6oBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    \"\"\"\n",
        "    This function performs a training step for the model on a batch of data.\n",
        "    Arguments:\n",
        "        inp: a tensor, the input to the encoder network which is a batch of \n",
        "              vectors of integers (indices of words) for sentences (questions) \n",
        "              with a shape of (batch_size, encoder_max_seq_len)\n",
        "        targ: a tensor, the real output that the decoder will use to learn \n",
        "              using teacher forcing. It has a shape same as `inp`\n",
        "              which is (batch_size, encoder_max_seq_len)\n",
        "        enc_hidden: a tuple of four tensors, the initial hidden states for the\n",
        "              encoder network, each of the shape (batch_size, n_units*2)\n",
        "    Returns:\n",
        "        batch_loss: loss for the given batch.\n",
        "        batch_acc: accuracy for the given batch.\n",
        "        batch_bleu: bleu score for the given batch.\n",
        "        batch_ppl: perplexity for the given batch.\n",
        "    \"\"\"\n",
        "    \n",
        "    loss = 0\n",
        "    ppl = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Run the input through the encoder and get back the encoder output\n",
        "        # and the hidden states of the encoder.\n",
        "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "        \n",
        "        # Set the hidden state of the encoder to be the initial hidden state\n",
        "        # of the decoder.\n",
        "        dec_hidden = enc_h\n",
        "\n",
        "        # Define the decoder input which is basically the '<start>' token,\n",
        "        # for every sentence in the batch.\n",
        "        dec_input = tf.expand_dims([text_tokenizer.word_index['<start>']] \n",
        "                                   * BATCH_SIZE, 1)\n",
        "        \n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        # looping over timesteps\n",
        "        for t in range(1, targ.shape[1]):\n",
        "\n",
        "            # Pass encoder output to the decoder along with the decoder\n",
        "            # input and initial hidden state and get back the predictions\n",
        "            # for this batch at the current timestep with the hidden state \n",
        "            # of the decoder\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, \n",
        "                                                 dec_hidden, \n",
        "                                                 enc_output)\n",
        "            # predictions shape: (batch_size, decoder_vocab_size)\n",
        "            # dec_hidden shape: (batch_size, units*2)\n",
        "            # attention weights (3rd output that has been discarded) \n",
        "            # shape: (batch_size, decoder_max_seq_len, 1)\n",
        "\n",
        "            # compute the loss\n",
        "            loss += compute_loss(targ[:, t], predictions)\n",
        "\n",
        "            # compute the perplexity\n",
        "            ppl += compute_perplexity(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    # compute the loss for the batch\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "    \n",
        "    # compute the perplexity for the batch\n",
        "    batch_ppl = (ppl / int(targ.shape[1]))\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # get gradients\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss, batch_ppl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPorzX-a9hIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nicely formatted time string\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return f\"{h}:{m}:{round(s,1)}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVTJQ7Pr9qwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j60ZMrdy9wtE",
        "colab_type": "text"
      },
      "source": [
        "## Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlMTloRX9yoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80284929-0160-4332-c09c-53e16e5cf3f9"
      },
      "source": [
        "print(hyperparameters)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'buffer_size': 25, 'batch_size': 10, 'steps_per_epoch': 2, 'embedding_dim': 20, 'units': 15, 'vocab_size': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaIriyGx948p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWOAcrLh-B18",
        "colab_type": "text"
      },
      "source": [
        "Let's train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FGAhGnW-C_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae4b50c1-29d4-4891-8155-b9c8d83276d2"
      },
      "source": [
        "EPOCHS = 30\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# to cache loss and perlexity over epochs\n",
        "cache = dict({'train_loss': [], 'train_ppl':[]})\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    for epoch in range(EPOCHS):\n",
        "        start_epoch = time.time()\n",
        "\n",
        "        # Initialize encoder hidden state\n",
        "        enc_hidden = encoder.initialize_hidden_state()\n",
        "\n",
        "        total_loss = 0\n",
        "        total_ppl = 0\n",
        "        \n",
        "        # Training the model using the training data\n",
        "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "            \n",
        "            # Train the model on current batch\n",
        "            (batch_loss, batch_ppl) = train_step(inp, targ, enc_hidden)\n",
        "\n",
        "            total_loss += batch_loss\n",
        "            total_ppl += batch_ppl  \n",
        "\n",
        "            # print loss and perplexity for current batch\n",
        "            if batch % 400 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{EPOCHS} - \"\n",
        "                      f\"batch: {batch}/{steps_per_epoch} - \"\n",
        "                      f\"loss: {batch_loss.numpy()} - ppl: {batch_ppl}\")\n",
        "        \n",
        "        # compute batch loss and perplexity\n",
        "        total_loss = total_loss / steps_per_epoch\n",
        "        total_ppl = total_ppl / steps_per_epoch     \n",
        "\n",
        "        # Log loss and perplexity to TensorBoard for current epoch\n",
        "        with summary_writer.as_default():\n",
        "          tf.summary.scalar('training_loss', total_loss, step=epoch)\n",
        "          tf.summary.scalar('training_perplexity', total_ppl, step=epoch)\n",
        "\n",
        "        # Save (checkpoint) the model every 15 epochs\n",
        "        if ((epoch+1) > 1) and ((epoch+1) % 15 == 0):\n",
        "          checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "          print(f'Saved checkpoint for epoch {epoch+1}/{EPOCHS} to: {checkpoint_prefix}')\n",
        "        \n",
        "\n",
        "        # cache the loss and perplexity for current epoch\n",
        "        cache['train_loss'].append(total_loss)\n",
        "        cache['train_ppl'].append(total_ppl)\n",
        "\n",
        "        # print loss and perplexity for current epoch\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS} - loss: {total_loss} - \"\n",
        "              f\"ppl: {total_ppl}\")\n",
        "        \n",
        "        print(f\"Time taken for epoch ({epoch + 1}): \"\n",
        "              f\"{time.time() - start_epoch} sec\\n\")\n",
        "\n",
        "execution_time = (time.time() - start_time)\n",
        "print(f'Elapsed time: {hms_string(execution_time)}')"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30 - batch: 0/2 - loss: 2.017299175262451 - ppl: 13.47641658782959\n",
            "Epoch 1/30 - loss: 1.9846668243408203 - ppl: 11.982429504394531\n",
            "Time taken for epoch (1): 23.718831777572632 sec\n",
            "\n",
            "Epoch 2/30 - batch: 0/2 - loss: 1.972826600074768 - ppl: 11.087916374206543\n",
            "Epoch 2/30 - loss: 2.0046708583831787 - ppl: 11.844911575317383\n",
            "Time taken for epoch (2): 0.04822731018066406 sec\n",
            "\n",
            "Epoch 3/30 - batch: 0/2 - loss: 2.057194709777832 - ppl: 12.75274658203125\n",
            "Epoch 3/30 - loss: 1.9707937240600586 - ppl: 11.701409339904785\n",
            "Time taken for epoch (3): 0.046846628189086914 sec\n",
            "\n",
            "Epoch 4/30 - batch: 0/2 - loss: 1.9706367254257202 - ppl: 10.714056968688965\n",
            "Epoch 4/30 - loss: 2.0766279697418213 - ppl: 12.557819366455078\n",
            "Time taken for epoch (4): 0.046835899353027344 sec\n",
            "\n",
            "Epoch 5/30 - batch: 0/2 - loss: 2.3318817615509033 - ppl: 14.975980758666992\n",
            "Epoch 5/30 - loss: 2.0427937507629395 - ppl: 12.538124084472656\n",
            "Time taken for epoch (5): 0.05072140693664551 sec\n",
            "\n",
            "Epoch 6/30 - batch: 0/2 - loss: 2.350975513458252 - ppl: 16.24897575378418\n",
            "Epoch 6/30 - loss: 2.115450143814087 - ppl: 13.816017150878906\n",
            "Time taken for epoch (6): 0.04800224304199219 sec\n",
            "\n",
            "Epoch 7/30 - batch: 0/2 - loss: 2.0285134315490723 - ppl: 12.042859077453613\n",
            "Epoch 7/30 - loss: 2.0071566104888916 - ppl: 11.695196151733398\n",
            "Time taken for epoch (7): 0.0477290153503418 sec\n",
            "\n",
            "Epoch 8/30 - batch: 0/2 - loss: 1.601021647453308 - ppl: 9.10585880279541\n",
            "Epoch 8/30 - loss: 1.962752342224121 - ppl: 12.141775131225586\n",
            "Time taken for epoch (8): 0.04724693298339844 sec\n",
            "\n",
            "Epoch 9/30 - batch: 0/2 - loss: 2.110541820526123 - ppl: 14.246302604675293\n",
            "Epoch 9/30 - loss: 2.0448031425476074 - ppl: 12.405418395996094\n",
            "Time taken for epoch (9): 0.04862165451049805 sec\n",
            "\n",
            "Epoch 10/30 - batch: 0/2 - loss: 2.210880756378174 - ppl: 14.772541046142578\n",
            "Epoch 10/30 - loss: 2.020559787750244 - ppl: 12.272174835205078\n",
            "Time taken for epoch (10): 0.05517220497131348 sec\n",
            "\n",
            "Epoch 11/30 - batch: 0/2 - loss: 2.082379102706909 - ppl: 12.904791831970215\n",
            "Epoch 11/30 - loss: 1.9975075721740723 - ppl: 11.74237060546875\n",
            "Time taken for epoch (11): 0.047405242919921875 sec\n",
            "\n",
            "Epoch 12/30 - batch: 0/2 - loss: 2.1246562004089355 - ppl: 13.264716148376465\n",
            "Epoch 12/30 - loss: 2.037851572036743 - ppl: 12.195796966552734\n",
            "Time taken for epoch (12): 0.0477137565612793 sec\n",
            "\n",
            "Epoch 13/30 - batch: 0/2 - loss: 1.8835909366607666 - ppl: 11.617298126220703\n",
            "Epoch 13/30 - loss: 1.9051741361618042 - ppl: 11.186628341674805\n",
            "Time taken for epoch (13): 0.046974897384643555 sec\n",
            "\n",
            "Epoch 14/30 - batch: 0/2 - loss: 2.263213872909546 - ppl: 14.004172325134277\n",
            "Epoch 14/30 - loss: 2.041640043258667 - ppl: 12.84500503540039\n",
            "Time taken for epoch (14): 0.05394458770751953 sec\n",
            "\n",
            "Epoch 15/30 - batch: 0/2 - loss: 1.917288064956665 - ppl: 9.94426441192627\n",
            "Saved checkpoint for epoch 15/30 to: /content/path/training_checkpoints/ckpt\n",
            "Epoch 15/30 - loss: 1.9700984954833984 - ppl: 11.603900909423828\n",
            "Time taken for epoch (15): 0.08842611312866211 sec\n",
            "\n",
            "Epoch 16/30 - batch: 0/2 - loss: 1.8107929229736328 - ppl: 9.615518569946289\n",
            "Epoch 16/30 - loss: 1.97683846950531 - ppl: 11.617731094360352\n",
            "Time taken for epoch (16): 0.04645657539367676 sec\n",
            "\n",
            "Epoch 17/30 - batch: 0/2 - loss: 2.0322654247283936 - ppl: 11.456852912902832\n",
            "Epoch 17/30 - loss: 1.9694759845733643 - ppl: 11.64588737487793\n",
            "Time taken for epoch (17): 0.04693436622619629 sec\n",
            "\n",
            "Epoch 18/30 - batch: 0/2 - loss: 1.9386199712753296 - ppl: 12.081616401672363\n",
            "Epoch 18/30 - loss: 1.9926848411560059 - ppl: 11.38992691040039\n",
            "Time taken for epoch (18): 0.05845522880554199 sec\n",
            "\n",
            "Epoch 19/30 - batch: 0/2 - loss: 1.7188777923583984 - ppl: 9.41492748260498\n",
            "Epoch 19/30 - loss: 1.9871864318847656 - ppl: 11.685280799865723\n",
            "Time taken for epoch (19): 0.04877138137817383 sec\n",
            "\n",
            "Epoch 20/30 - batch: 0/2 - loss: 1.8875569105148315 - ppl: 10.043959617614746\n",
            "Epoch 20/30 - loss: 1.8575477600097656 - ppl: 10.797879219055176\n",
            "Time taken for epoch (20): 0.04614090919494629 sec\n",
            "\n",
            "Epoch 21/30 - batch: 0/2 - loss: 2.0566515922546387 - ppl: 12.042034149169922\n",
            "Epoch 21/30 - loss: 2.0925140380859375 - ppl: 12.525663375854492\n",
            "Time taken for epoch (21): 0.04776358604431152 sec\n",
            "\n",
            "Epoch 22/30 - batch: 0/2 - loss: 1.8977984189987183 - ppl: 10.908269882202148\n",
            "Epoch 22/30 - loss: 2.0620102882385254 - ppl: 12.284165382385254\n",
            "Time taken for epoch (22): 0.05014371871948242 sec\n",
            "\n",
            "Epoch 23/30 - batch: 0/2 - loss: 2.1882903575897217 - ppl: 13.813830375671387\n",
            "Epoch 23/30 - loss: 2.0041275024414062 - ppl: 11.610051155090332\n",
            "Time taken for epoch (23): 0.04648542404174805 sec\n",
            "\n",
            "Epoch 24/30 - batch: 0/2 - loss: 1.6514290571212769 - ppl: 9.359807014465332\n",
            "Epoch 24/30 - loss: 1.8603034019470215 - ppl: 10.416216850280762\n",
            "Time taken for epoch (24): 0.04887223243713379 sec\n",
            "\n",
            "Epoch 25/30 - batch: 0/2 - loss: 1.663010835647583 - ppl: 8.969212532043457\n",
            "Epoch 25/30 - loss: 1.9141134023666382 - ppl: 10.588766098022461\n",
            "Time taken for epoch (25): 0.04779624938964844 sec\n",
            "\n",
            "Epoch 26/30 - batch: 0/2 - loss: 2.1421303749084473 - ppl: 12.742362022399902\n",
            "Epoch 26/30 - loss: 1.923407793045044 - ppl: 11.385011672973633\n",
            "Time taken for epoch (26): 0.04792475700378418 sec\n",
            "\n",
            "Epoch 27/30 - batch: 0/2 - loss: 2.0606942176818848 - ppl: 13.196413040161133\n",
            "Epoch 27/30 - loss: 1.9588675498962402 - ppl: 11.726709365844727\n",
            "Time taken for epoch (27): 0.04911065101623535 sec\n",
            "\n",
            "Epoch 28/30 - batch: 0/2 - loss: 1.8410379886627197 - ppl: 10.84286117553711\n",
            "Epoch 28/30 - loss: 1.7665140628814697 - ppl: 9.732939720153809\n",
            "Time taken for epoch (28): 0.05150580406188965 sec\n",
            "\n",
            "Epoch 29/30 - batch: 0/2 - loss: 1.8584372997283936 - ppl: 9.616870880126953\n",
            "Epoch 29/30 - loss: 1.823596715927124 - ppl: 9.960529327392578\n",
            "Time taken for epoch (29): 0.047753095626831055 sec\n",
            "\n",
            "Epoch 30/30 - batch: 0/2 - loss: 2.1083905696868896 - ppl: 13.026219367980957\n",
            "Saved checkpoint for epoch 30/30 to: /content/path/training_checkpoints/ckpt\n",
            "Epoch 30/30 - loss: 1.935920238494873 - ppl: 10.94985580444336\n",
            "Time taken for epoch (30): 0.0621490478515625 sec\n",
            "\n",
            "Elapsed time: 0:0:25.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ-C40nD_AmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlNi0wnW_WMz",
        "colab_type": "text"
      },
      "source": [
        "Plot metrics over time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpdmHsk9_XNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_metrics(cache):\n",
        "    \"\"\"\n",
        "    Plots loss and perplexity over epochs:\n",
        "    Arguemnts:\n",
        "        cache: a dictionary that contains values over epochs\n",
        "               for both loss and perplexity.\n",
        "    \"\"\"\n",
        "    \n",
        "    # since, items of cache['train_loss'] and cache['train_ppl']\n",
        "    # are tensors, so let's extract values from these tensors.\n",
        "    loss = [tensor.numpy() for tensor in cache['train_loss']]\n",
        "    ppl = [tensor.numpy() for tensor in cache['train_ppl']]\n",
        "    \n",
        "    # Plot the loss\n",
        "    plt.figure()\n",
        "    plt.plot(loss, label='Training Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot the perplexity\n",
        "    plt.figure()\n",
        "    plt.plot(ppl, label='Training Accuracy')\n",
        "    plt.title('Perplexity')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyNeW_jrAJww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "c75a80c3-524f-41b2-9623-39a437307247"
      },
      "source": [
        "plot_metrics(cache)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aAjcDcfAfRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klWv4-E3A00W",
        "colab_type": "text"
      },
      "source": [
        "## Inference Mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFhk4pbVA96p",
        "colab_type": "text"
      },
      "source": [
        "Now, it's time to try the model and ask him a few questins :)\n",
        "\n",
        "Helper function: to clean the text the user has entered."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DCxpQNJA_MT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    This method takes a string, applies different text preprocessing \n",
        "    (characters replacement, removal of unwanted characters, removal of extra \n",
        "    whitespaces) operations and returns a string.\n",
        "    Arguments:\n",
        "      text: a string.\n",
        "    Returns:\n",
        "      a cleaned version of text.\n",
        "    \"\"\"\n",
        "    \n",
        "    import re\n",
        "    \n",
        "    text = str(text)\n",
        "    \n",
        "    # REPLACEMENT\n",
        "    text = re.sub('\\\"', '\\'', text)\n",
        "    text = re.sub(\"“\", '\\'', text)\n",
        "    text = re.sub(\"”\", '\\'', text)\n",
        "    text = re.sub('’', '\\'', text)\n",
        "    text = re.sub('\\[','(', text)\n",
        "    text = re.sub('\\]',')', text)\n",
        "    text = re.sub('\\{','(', text)\n",
        "    text = re.sub('\\}',')', text)\n",
        "    text = re.sub(\"([?.!,:;'?!+\\-*/=%$@&()])\", r\" \\1 \", text)\n",
        "\n",
        "    pattern = re.compile('[^a-zA-Z0-9_\\.\\,\\:\\;\\'\\?\\!\\+\\-\\*\\/\\=\\%\\$\\@\\&\\(\\)]')\n",
        "    # remove unwanted characters\n",
        "    text = re.sub(pattern, ' ', text)\n",
        "    \n",
        "    # lower case the characters in the string\n",
        "    text = text.lower()\n",
        "    # REMOVAL OF EXTRA WHITESPACES\n",
        "    # remove duplicated spaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    # remove leading and trailing spaces\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-XkZ9OkCprd",
        "colab_type": "text"
      },
      "source": [
        "The following fucntion passes a sentence that has been entered by the user to the model and returns its answer along with the input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEJkOl2jCqor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    \"\"\"\n",
        "    This function takes a sentence (question) and returns the model's output\n",
        "    (answer) to it.\n",
        "    Arguemnts:\n",
        "      sentence: a string.\n",
        "    Returns:\n",
        "      result: a string, representing model's output to the input.\n",
        "      sentence: a string, the input sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    # clean the input sentence (question) to prepare for the encoder\n",
        "    sentence = clean_text(sentence)\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "\n",
        "    # tokenize the input sentence and pad zeros if its length is less than\n",
        "    # maximum sequence length.\n",
        "    inputs = [text_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    # initilize the hidden state of the encoder\n",
        "    enc_hidden = (tf.zeros((1, units)), \n",
        "                  tf.zeros((1, units)), \n",
        "                  tf.zeros((1, units)), \n",
        "                  tf.zeros((1, units)))\n",
        "    \n",
        "    enc_output, enc_hidden, enc_c = encoder(inputs, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([text_tokenizer.word_index['<start>']], 0)\n",
        "    \n",
        "    # generate answer, where the maximum length for the answer is equal\n",
        "    # to max_length_targ=32\n",
        "    for t in range(max_length_targ):\n",
        "      predictions, dec_hidden, attention_weights = decoder(dec_input, \n",
        "                                                          dec_hidden, \n",
        "                                                          enc_output)\n",
        "\n",
        "      predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "      result += text_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "      if text_tokenizer.index_word[predicted_id] == '<end>':\n",
        "        return result, sentence\n",
        "\n",
        "      # the predicted ID is fed back into the model\n",
        "      dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOM6JmDUC4O2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def answer(sentence):\n",
        "  \"\"\"\n",
        "  This function takes an input sentence by the user and prints the \n",
        "  model's answer along with the user's input sentence.\n",
        "  Arguments:\n",
        "      sentence: a string.\n",
        "  \"\"\"\n",
        "\n",
        "  result, sentence = evaluate(sentence)\n",
        "\n",
        "  print(f'INPUT: {sentence}')\n",
        "  print(f'CHATBOT ANSWER: {result}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T04u9VIZD5js",
        "colab_type": "text"
      },
      "source": [
        "Let's try to talk to the chatbot:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwrgxjNQD6i6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer(' me')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFjXceU3ER2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}